# The Role of Modeling in Scientific Theory

**Presenter:** Patrick Stinson
**Source:** YouTube Video Transcript
**URL:** https://www.youtube.com/watch?v=YLoXaLJ3Bgs

---

## Introduction

This is going to be the fourth time that I've presented on this modeling concept in this meeting, and it's going to be the last one. I think I've gotten clear enough on this now to move off of this hypothetical stuff—the basic rules for research, basic rules for science, for this systems science thing, whatever that is—and just start doing the dirty work of plucking data and all that.

I'm going to make an argument here about what is standing in the way between Bowen theory (as it is—the theory itself, the scientific part, or the part that ought to be scientific) and science proper. This has been a hunch I've been trying to work out through many different presentations, not just in this meeting. But I think there's something very specific that is standing in the way, and I'm going to try to nail it down.

It's going to be the relationship between model and theory.

The theory part is pretty strong in this scene, if you will. I think the modeling part is not super clear, and it's understandable because it's the hardest part.

## The Thesis

The thesis I'm throwing down here, which I'm going to unpack, is that **Bowen theory does not move towards mainstream science without predictive models**.

Now that's not entirely right, actually. I'm not going to say that's the whole story. I could hypothetically imagine someone reading some paper about Bowen theory, and suddenly, somehow in their mind, the view comes into place—and all of a sudden, not only human systems in general, but the family in particular, starts to look like it has a functional structure, a behavioral structure, not just a physical structure like the dominance hierarchy of reproduction (the physical structure or less).

Maybe that'll happen. I don't know. But I think it's probably reasonable for mainstream researchers to pass this theory by until there are predictive models that form the argument for the theory. My thesis here is just that it's missing—but I've got to unpack exactly what I mean by *model* to make that clear.

I'm elevating the term "model" to a **technical term**, which means there's a function I'm trying to capture here. This might not be the right word for it, but any critique on the thesis ought to better articulate the function I'm getting at, the problem I'm getting at, rather than the exact terms used.

By technical term, I mean it has a precise definition and a place in the chain of the scientific process.

---

## Part 1: The Problem

### The Universality of Prediction in Science

You can debate all day about what science is. You can debate all century about what science is, and that's always going on and that's good. But there's a piece of this I'm trying to pull out in particular.

There is a universality to the function of science—the phenomenon of science over the evolutionary history of the species. There is a phenomenon that has emerged called science that is not just a human idea, but something that has tied into natural laws itself in practice.

There is a certain kind of value that comes—that is instantly recognizable—when you have a certain kind of knowledge that predicts. **Well-replicated predictive models are instantly accepted.**

That is a particular criterion that science has uniquely brought to the world. At least the systematic explication of that in practice is a product of science.

Here's an example: I heard this on the news a couple of years ago—the first time this dawned on me. When the average Joe is watching the news, and there's a story that some 17-year-old kid came up with a way to detect lung cancer with a computer program that can do it with high accuracy—everyone gets it. You don't have to go through the long-winded academic critique. That has to happen in the background, but if you come up to any primitive, non-intellectual culture and say, "Yeah, the sun comes up every day. Tomorrow it's going to come up and it's going to have something in front of it, and it's going to be dark all day"—even a culture that doesn't have science can get that, especially if you do it two or three times.

There's a basic thing happening here that is universal—deeper than a human idea. There's a reason that neuroscientists today are using prediction as a main principle in behavioral theory—the lack of error. At the academic level, that has a particular value and potency that automatically is recognized.

At the academic level, you do an experiment, get it replicated, get it replicated five more times—replicated by people who really don't want to replicate it, in different domains of expertise. That's really strong validity.

But there's another level: where the knowledge or the tech or whatever the prediction was—whatever the discovery was—is actually put into practice all over the place, in great numbers, so reliably that you forget about it. You don't even have to think about it. It doesn't take calories to run through the knowledge in order to use it.

The example for that: I'm talking over Zoom, I've got my phone—it manifests millions of such predictions simultaneously every second, and we don't even care because we don't have to. It's done. That's the ultimate level.

### The Downside: Erosion into Dogma

The downside of this is that while prediction has prestige and deserves it, just like anything, it's easy for it to erode into a dogma—for the prestige to be "shoplifted" (to use that funny word from Jeremy Warrer). It's exploited without the rigor behind it.

All the terms and words like "science," "prediction," "fact," "theory," "variable," "data," etc., carry the value and can be used in ways that do not pertain to that universal phenomenon—that universal criterion of well-replicated, non-falsified prediction.

This is happening in the academy. I don't know if any of you have seen the efforts (there were at least two, maybe three) to hoax various sociological journals, where such pseudoscientific jargon was used in these isolated philosophical areas of academia. They had these journals, and the jargon was so specialized, so esoteric, so incoherent that the hoaxers came along and just made up experiments using new jargon that fit the pattern of the old jargon but wasn't the same words—and they were accepted and applauded in these "scientific" journals. That just shows that the journal has lost its connection with the scientific part of it. Maybe the journal has another function, and that's fine, but there's a prestige that comes with the universal characteristic—the core scientific principle of prediction—that is easy to get lost, easy to attach onto without actually having the rigor behind it.

### Erosion within Bowen Theory

Throughout the culture, tradition, and lexicon of Bowen theory (because I don't think the tradition is the same as the theory—there's how people approach the theory and then there's the theory itself, and then there's the natural phenomenon the theory describes, all different things), I think it's easy for words like "fact," "variable," and "data" to be attached to the prestige while eroding.

We're all just doing the best we can, myself included, to stick to the precision. But it's easy to have, for example, a timeline of "facts" that are not facts. And then you have to get into what's a fact and all that.

My whole point: there's a particular function that, if it doesn't have its own technical term, is easy to get lost in the weeds. You have no way to point to it.

Like "theory"—this all comes from Bowen's *Odyssey* chapter—where if the term "theory" becomes equated with "opinion," then you have no way to actually describe something that has evidence behind it, that is moving closer towards greater validity. That's frustrating and not very productive.

The same is probably true for "variable" and "data." If you use "variable" for anything that has an influence in a complicated phenomenon like family, what word do you use for something that's actually quantitative? For example, the variable of differentiation itself, the variable of anxiety—what do you have for the factual variable that would be used when you have a variable that really is quantitative, even if it's just binary (yes or no), and not a numerical/integral/ratio variable?

I think "factor" is a good word for that, but I'm just pointing out the problem.

Same thing with data. "Data" has this feel like it's part of this universal prestige, but what is it when it's really loose and disorganized, or only in a pile of handwritten notes? Is that data, or is that just information in a heap?

This erosion is a piece of the broader problem that the modeling technical term ought to solve.

### Research vs. Formal Research

I just got clear on this the other day. The thesis here pertains to **formal research**, and that is not necessarily the same as research on one's family—though that's not quite right either. That's why I have this slide.

The effort ought to be to apply rigorous thinking as much as possible to one's own functioning and how that functioning is modulated in the context of a broader biological system. Yes.

However, the product of that research—and if any of you have ever seen a counter-example for this, please tell me so I can stop attempting to differentiate in this way—I have never seen an effort, an actual product of an individual effort of differentiation of self in the research vein, that produced something that was acceptable and peer-reviewable in a scientific journal, that had some kind of generalizable (i.e., move beyond the case study) outcome. Even though case studies have their place, I've never seen an individual differentiation effort get there.

That doesn't mean the scientific principles aren't being applied and the effort isn't going there. But those are not necessarily the same thing in terms of outcome, because the standards for formal academic research are extremely high compared to "has this had an impact on my life."

It took me a while to get clear on what I just said, because Bowen the person made this amazing contribution—one of the things that pulls me back to this theory constantly—which is an integrated view to house all this stuff. I think it's integrated by the research mode, more or less: you can be a professional, you can have a client, and each one of you is in effect doing something similar, organized by this universal principle, not only in the case material individually but in your own functioning at the same time.

That's pretty incredible. But I think it's also easy for that research attitude and the outcome of research on one's own family to appear the same as formal research. The only reason I'm going down this road right now is because I think the presence of a well-replicated predictive model is probably a big difference between the two.

---

## Part 2: Defining the Terms

To sum up the problem: I think descriptions about refining theory, researching theory, thinking theory, using it to guide professional practice, using it to guide a grow-up effort for oneself—all of that is pretty darn rich already. But the modeling part hasn't been defined very clearly.

So that's what I'm going to try to do here: just the basic scientific principles for this, regardless of what model is built, and how that relates to this unique challenge that Bowen put out there for the social sciences. He raised the bar in some ways on basic scientific criteria, and that also created this integrated view I was just talking about—but it also has this problem of not having the modeling phase or the proving phase being clear.

### The Model

The ultimate goal—the thing that automatically raises eyebrows in the mainstream—is a **predictive model**, a well-replicated predictive model. What exactly do I mean?

I mean a device, extra-cranial (that's the whole thesis for this thing), that calculates predictions. The actual computation is done by the model, and it occurs with data—it takes data as input.

The whole point of this is to get it outside of the brain. Bowen used the phrase "outside the brain" in his videos, and I think it's great, because the calculation can occur in the brain but appear to be objective. The reason I'm pointing this out is that **the model is something explicit, tangible**—even if it's a set of instructions on paper. The point is to get the person's computational/evaluation function out of the head onto something else, to get it automated.

This is not a complicated concept. It's almost hilarious that I'm having to say any of this, or having to clarify it like I've been doing for four years personally, because none of this has to be said in hard sciences. This is all just taken for granted—nobody even has to think about it because all thinking is on models. There is theory, but the conversation occurs around models.

**Examples:**
- A regression equation is a model—the computation occurs outside the brain
- Weather models—I pull up the weather for vacation evening, and there's an incredible amount of models that are all automated. Nobody's doing that mentally, and they're not super great, but they're sure useful
- Ian Couzin's fish models—the model itself is the code, not the theory that he described, not the three variables per se. That would be part of the theoretical level until it is embodied in a physical form or in any way that's automated, that gets it out of the brain. It's actually in the code.

So that's the ultimate goal.

### The Data Model

There's a sub-model, a sub-task. The **data model** defines what is tracked.

In statistical regression, whatever is fed into the regression model—you've got an independent and a dependent variable—the independent variable, where does that come from? A data model formalizes how it's tracked. It's just the columns in the table.

This is such a simple idea, but I think both of these—predictive models and data models—are missing from Bowen theory, and they are a huge thing that's missing. Even the data model, which would be the prerequisite for a predictive model.

**Questions a data model must answer:**
- What exactly is it that you have to record to capture what is described in theory?
- What exactly is it that you have to record that would apply across every case?
- How do you track triangles? And not just track mentally—this is the whole point—how do you systematically record the data required to feed into an automated predictive model to tell you what will happen in the next step?
- Even in the very next step: if the triangle is in this state, and you have only one triangle, and you don't have anybody else around, and it's really intense—what is the next step that will occur? How would you even record the steps that had come prior?

This part I haven't seen. And this goes back, as far as I know, to Newton's tradition. This goes back to "I believe I have observed something; how can I prove it, more or less?" Well, you have some way of calculating what will occur in the future based on what has occurred in the past, and you provide instructions to replicate the experiment so that other people can not only run the model but can go collect their own data according to your instructions and then run the model.

But how do you do that without the data? And I mean the quantitative data, the hard data. The family diagram is not sufficient for that. It's a brilliant thumbnail sketch that's very useful, and I think it's actually a prototype for what would become a data model—and that's what my effort in the app is about. But this is a more difficult task.

---

## Part 3: Implicit vs. Explicit Models

### The Implicit Model Concept

Last year in this meeting, I presented a theoretical concept called the **implicit model**. Regardless of whether that made any sense at all, this is a good application of the idea. I wasn't clear enough on the thesis I'm making today last year. It was still more abstract, and that's why I didn't have an example, but this is a good application of it.

When I was saying the predictive model at its best is outside of the head—that the calculation making the prediction is occurring outside your head—that means it has become **explicit**.

While it's still occurring—if the statement "theory predicts X" is verbal, and you're just thinking it, and you're visualizing the "quote data" in order to produce that prediction, and then you just say the prediction—the calculation is **implicit**. It's happening in the head.

By my definition (maybe there's a better definition), this appears to be the state of the art of Bowen theory: predictions are implicit. They're using an implicit model.

For what I know of basic research principles, you can actually have predictive implicit models because you can have inter-rater reliability on things that are subjective, and that can be durable. For example, no one has simulated human vision yet, and we are all able to predictably move with the same visual input through the world, more or less. So there is agreement that works without an explicit model yet, and that's great.

But I'm trying to point out what an implicit model is, because I think it's useful in defining what the state of the art of Bowen theory is. Yes, predictions are made—theory does predict in a manner of speaking—but that's not the same as a model doing the predicting for you, or me, or someone. So that's inherently less subjective, or at least it doesn't have the universal hook that a blind, well-replicated explicit model will have. It doesn't automatically raise eyebrows. It isn't universal.

Last year, I was using "implicit model" and "explicit model" as a theory of behavior, a bit of a deeper way to do it, but it's essentially the same idea.

### The "Concepts from the Human Brain" Problem

Back to Bowen's contribution. He made this key contribution (still applicable, I think) about defining how theory works—how *a* theory works, not just Bowen theory, not just family systems, but what is the term "theory," what is the term "science," what is "hypothesis," what are the problems (kind of like I laid out before) and how it can go awry. That's really rich. I don't have anything to add there. It's beyond my ability, honestly.

Attached to that was this phrase: **"concepts from the human brain."**

I keep running up against this when thinking at the modeling level (as opposed to, or in conjunction with, the theory level). Bowen's goal was to come up with words that described what was being observed, that fit the thing being observed as much as possible, as opposed to some pre-existing model or some idea that was more a function of the brain than what was observed.

There's a problem that occurs when an idea isn't grounded in the feedback loop of observation, testing, and scientific rigor. There's a problem that occurs when an idea is just slapped onto something and called a theory.

The poster child for that within this culture/tradition/lexicon is **general systems theory** and **cybernetics**. By the way, I think these terms serve a particular function still, but they need to be replaced because they are horribly out of date.

When I started reading Bowen theory literature seriously in early grad school and started talking about general systems theory, it came up in everyone's stuff since then. It took me like three years to figure out why, because it seemed to me like there was a particular point of view protected by these terms. And I think that's actually still true.

There was a fallacy here. From the point of view of the way that Murray Bowen decided to proceed with theory, there was a problem with simply taking a mathematical model (in the case of general systems theory) and then applying it to a natural phenomenon and calling it a "natural system theory."

Bertalanffy and Laszlo (two big characters in the general systems scene) use the term "natural systems" all the time. That's not the same as the pure inductive effort, which (as I've clarified it) is:
1. Collect observations
2. Keep them as factual as you can
3. Merely describe the observations in some way that's clear enough that you can predict future observations
4. Keep to what's observed and try to keep the terms simple

There's a problem that arrives in the development of theory when there are all these different terms coming from all these different places. The first example was Freud, taking terms from mythology and culture.

**However:** At the modeling phase, it *is* concepts from the human brain. It *is* an engineering effort.

I think this phrase "concepts from the human brain" can end up functioning as a **bogeyman** until this modeling phase is actually defined and clarified. Because you *have to* use concepts, ideas, and mathematical theorems in models in order to predict.

The difference being: instead of just taking a model and slapping it onto a phenomenon and saying it's a theory (which is really an analogy, not a theory), and then using that to guide some kind of practice—you're **testing the model** with blind, falsifiable predictions so that it gets closer and closer to not only the theory but the phenomenon itself.

If that's the goal, then it doesn't matter what the model is to start with. But if there is no testing, then you run into the "theory and therapy" problem—where all effort is on changing without understanding and predicting. When you can understand and predict, then how to change it—the options on how to change it—automatically open up.

This is evident in the countless articles and journals on systems thinking and systems philosophy that are not the same thing as this natural systems theory domain that Bowen was getting at. If you read through case studies and applications of a particular philosophical application of a systems theory, it's really just hammering on the professional organization, for example, trying to get some kind of an outcome without having a way to predict what's occurring already and what would happen in the future.

### Markovian Predictions: An Example

Here's an example that applies to the family system, where this problem with the phrase "concepts from the human brain" comes up.

I've said a few times in a few meetings (and I'm 60-75% sure) that the kind of prediction that occurs in an emotional system—whether it's the family or not—would be a **Markovian prediction**.

A Markovian prediction is where, given where the state of something is in the present moment, there is a limited number of pathways forward. That's a Markovian distribution.

You might say: if I'm giving a presentation on model building, there is a limited number of things I'm going to talk about (based on my previous slides). I'm not going to go off and start talking about music theory (probably), unless there's a long way to get there. I'm not going to go off and start talking about some art project. There's a reasonable scope. The distribution—the chances that I do choice A, B, C, or D—are not the same. There's a limited number of options moving forward, and there are higher chances than others about which way I'll go.

**An example from the family emotional system:** If I'm on the outside of a triangle at the present moment, what are my options going forward? You might correct me on this, but an example scope of options could be:
- I could try to move to the inside
- I could further intensify my outside position
- I could differentiate
- Maybe there's something I'm not thinking of

The point is there's a limited number of options on what to do next. But the chances that I'll do one over the other are not the same. There's a distribution of probability and a limited number of options—a Markovian probability.

As a professional, ten sessions in, I'm thinking, "Well, I know a little bit about what's going on here, at least in this nuclear family." Someone says, "Well, what should I do? Should I confront my wife about this?" or whatever. And yeah, it's correct to say, "I don't know what you should do"—that's technically accurate because there is a set of probabilities with a set of moves that can be made and a set of outcomes. You don't know which one. And then each one of those bifurcates off into its own set after you've made that next move. So it gets complicated fast.

But just in terms of the next step, you really don't know which one should be taken. However, there is some weighting sometimes to which one is more likely to have some kind of an outcome. It's a fuzzy probability. I think that's a system's prediction.

The point being: this is a mathematical concept. This might not deserve a place in theory, but it absolutely deserves a place in the model.

### Two Levels of Conversation

There are two levels of conversation (or maybe five, since I've got five here). Right now, I'm talking about the **theory level** and the **model level**. Those have slightly different boundaries.

The theory level is a verbal thing. It literally has concepts, which literally by definition are the brain's best effort at making sense of what's going on. "Concept" is a psychological concept—I just used it. That is probably a verbal and literary exercise/effort/project.

The modeling one is separate. I'm trying to differentiate the modeling phase out as something distinct. It has its own rules. You have to start getting into engineering ideas in order to produce an explicit model.

Bowen came along and made this phenomenal contribution: that theory is often implicit and must become explicit. It was a similar contribution at the theoretical level to what I'm trying to poke at through the modeling level.

If you break down what's behind one of the popular therapies—what's the theory? There's always something in there guiding the therapy or the ballistic missile effort. And if it's implicit, it can't be tested, it can't be refined. It has to be formalized, laid out, and gone through the process of rigorous critique. Bowen nailed that one, in my opinion.

**My whole thesis:** Modeling in Bowen theory is often implicit and must become explicit—in the broader formal research sense. And if possible, in the personal effort of differentiation. Who would argue with that? Good grief, if you can do that, go for it. But I think it's too complicated for that.

---

## Part 4: The Chain of Command

This is how the whole thing fits together, going back to Bowen's definitions in the *Odyssey*.

1. **Natural Phenomenon** — You'll never know it perfectly, but you can get closer and closer to it
2. **Observations** — Hopefully factual, though observations can be subjective. You lose accuracy at each step going down from nature to observation. Observations never quite capture nature because they are low-resolution sampling. If you go out and collect weather data at points in time, you missed all the stuff that happened in between the points. That's one way you lose precision.
3. **Theory** — In the natural systems inductive mode (as opposed to the deductive general systems art, or whatever), theory is an effort at explaining the observations in a manner that would also fit future observations. This is all covered in Bowen's writing and everyone that came after him.
4. **Model** — The modeling phase is an attempt to explicate theory. The model is the closest thing you can get to the theory itself, but it's not going to be quite the same. This is immediately apparent if you try to build a formal model of something that is a conceptual thing like a theory. The model is always going to be imperfect—otherwise it would be called the theory. Same thing with observations, same thing with nature.
5. **Interpretations** — Based on the output of the model, and hopefully there's a feedback that goes back and updates the theory

The updating goes up all the way to observation, not necessarily to nature. But the precision goes down as you go down the chain, and I'm adding the model step.

All of this is so verbose on such a simple idea that it's almost funny that I'm saying it, or that I even had to clarify all this stuff. But all of this was driven by particular problems that came up in conversation.

### How to Build a Model

I have a model I'm working on. That's not the point of this presentation—what the model is. But I have a model I'm working on, and I ran into a lot of these problems trying to figure out:
- Why am I not just using the words that are in theory?
- Why isn't that just the model?
- Why can't I just sit down and write out a model of the family?

I was watching Ian Couzin's presentations on fish—it seemed like that would be possible. But then when I actually did it, it was clear that a lot of work had happened to do that, even to make a speculative model.

**The goal** (in the chain of command going up): the next step up for model is theory. The goal for the model is to test the theory. The goal for the theory is to test observations (you could say), and so on.

**The design of the model, the building of the model, is guided by theory—but it can't contain the theory.**

This is the subtle thing I probably won't even be able to get across today, but I'm sure of this. The model, if it's going to test the theory, can't actually contain the theory. It would contain the constituent parts of the theoretical hypothesis necessary to test the hypothesis.

If you want to go out and see if triangles occur, you don't go count/record every time a triangle occurs. You have to have criteria for a triangle: What is a triangle? What are the parts that make up the triangle? And then: when are those parts *not* a triangle? That's the main thing that has to be separated out. Otherwise, you're just confirming your hypothesis.

**Guided by theory, but can't contain theory.** I think this is the main thing that can be confused—having a model that tests theory but actually assumes theory is fact in the first place and always confirms it.

As I was designing this model I'm about to show, I realized it requires total clarity about the theory. I think that the modeling phase, if it's going to be successful—the more clear on theory a person is, the more successful the modeling phase is—because it is a best-guess representation of what's described in theory.

---

## Part 5: An Example Model for Tracking Triangles

I was trying to come up with a model of how to track triangles—just triangles, the simple thing—in a universal format, a data model. I spent I don't know how many years, but definitely during an intense three-month period, I was going through all these mental loops trying to define what the observables are for the theory in a way that could be fed into a computer. Not as numbers, but just yes/no type of binary states for data. Is this person in this position? That kind of thing.

I had to get really clear about it. And I will now argue that because of that, I don't think the theory itself has been formally clarified on paper explicitly in this way—enough—because when you have that, you automatically have debate about it. And I have never seen that debate.

### The Research Question

**Do triangles occur with sufficient tension?**

That's something straight at the conceptual level from theory—a verbal question.

The data model, as I said, must be guided by theory but not contain theory. It has to be falsifiable.

**What are the parts of the triangle that you would track that would:**
- Give rise to a triangle under enough tension
- Not give rise to a triangle in its respective phases with sufficiently low levels of tension

So that it's falsifiable?

That was the clarity part. I thought, "Wait a second, it's not obvious what to track for that." I think there is agreement about this, but it's implicit and at an intuitive level.

### The Atomic Operation of Bowen Theory

My best stab at this (and I don't know that this is right because it's just a model—not tested yet): I had to define what I'm calling the **atomic operation of Bowen theory**.

The atomic operation is either a **toward** or **away** move:
- There's always one mover
- One or more recipients perceive the move (I haven't worked that part out yet fully)
- These occur over time—you can track that sort of thing
- **Toward** = increased contact
- **Away** = decreased contact

I'm not totally sold on those definitions, but it's a definition to start with.

You also track **arousal**, which is my best guess at throwing down an explicit model definition for anxiety. Again, losing precision in the process, but trying to narrow it down so it can be tracked.

### What Is a Triangle?

If that's the atomic operation—the indivisible thing that's happening—what is a triangle? I had to come up with a definition for that. So there goes another couple of months of mental gymnastics and talking to people and failing and succeeding.

The definition I came up with (not perfect, but modelable):

> **One move that is simultaneously a move toward one and away from another (or more)—one or more on either side—a simultaneous move that is toward one and away from another.**

I'm not even assuming that it's compensatory yet—that one in/away is compensating for another. That came in the problem of defining differentiation, and we'll leave that out.

The point: this is different than just thinking at the conceptual level. This is finding terms that kind of get there—to describe what's in theory—but well enough to track data and test. This required achieving a level of clarity that I don't think I've gotten there yet, but it requires clarifying what's already in theory explicitly. And then you've automatically got a debatable triangle definition.

### From Data Model to Predictive Model

This is just the data model. The goal is the predictive model, and that is a whole separate bag of snakes.

I worked with a couple of data scientists on this—computer scientists in their mid-20s, super sharp and motivated—on even just coming up with something that would plausibly work for a predictive model for this kind of data.

I understand it, but I don't think I could implement it without their help. It relies on modern Markovian probabilities that predict what the next move would be given the previous series of moves—the time series. It's complicated, but the point is that would have to be a computer program that would read in these sequences captured over many, many years of emotional process.

### The Data Model Visualization

This is just what that particular model looks like. This is a data model—it just says what is tracked:
- Tracking a series of moves over time
- A particular definition for what kind of move is happening
- One person who's making a move
- One or more recipients of that move
- The change in arousal in response to that move by any one of the people within the scope of the vignette

It took a lot of work to get here. I don't know that this is right. I've got three vignettes recorded from my family, and honestly, the hard part in the future is collecting data. I don't know how I'm going to collect data. But so goes the project.

There are all kinds of problems with this:
- How do you scope the vignette?
- How do you know who to include?
- Do you include the whole family?

Well, I don't know, but I'm just tracking vignettes that can be tracked. All these questions have to be answered with lots of data.

But I do think this is data—it's closer to data than just notes. Because it has a data model that is more objective than something that just occurs in discussion or on a family diagram.

You could totally critique:
- The objectivity of the "move" thing
- Whether arousal is an accurate representation of anxiety (I don't think it is, but you sacrifice in modeling—that's part of the deal, and then it gets updated later)

The whole point: back to this chain of command thing—this modeling phase is a separate conversation that has separate rules. The rules, if you will, for defining theory and thinking about theory and implementing theory are pretty darn rich and good under the Bowen theory literary tradition (maybe that's a good term—literary tradition, at least you have something concrete, the literary publications). It's really pretty good.

But the modeling part isn't there. I don't think Bowen got into this, and I'm not saying he didn't because I don't know—I don't know what's in the archives. Maybe he had a data model. I'd love to go see. I've actually got a proposal in to go see, because I think that question needs to be answered.

But here's where I'll tie it up: **If Bowen's model was not only implicit—if there was an explicit data model, if there was an explicit predictive model, even if it was just rules on paper (because that can be explicit too, so long as it doesn't require a calculation in the head)—and it actually predicted what he said he was predicting, then that would be the topic of conversation automatically, rather than the story of the NIMH study** (which is the topic of conversation usually).

Whenever I've written about Bowen theory, I've always had to start with the story for people who don't know Bowen theory, because that places it in context. But as far as I know—and I think as far as any mainstream researcher—those models were implicit. Because, you know, where are they?

And so the next step would be to produce them.

---

## Conclusion

I'm pushing this out as an argument. I think this is right, but how do I know? You never do.

There's a particular function I'm getting at with each one of these. These all came out of real-world problems. This is my attempt to solve a problem—or a series of problems—that I think are natural to happen given a topic of such complexity.
